{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe9056a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T13:42:11.406934Z",
     "iopub.status.busy": "2024-09-05T13:42:11.406372Z",
     "iopub.status.idle": "2024-09-05T13:42:26.553035Z",
     "shell.execute_reply": "2024-09-05T13:42:26.551663Z"
    },
    "papermill": {
     "duration": 15.158113,
     "end_time": "2024-09-05T13:42:26.555842",
     "exception": false,
     "start_time": "2024-09-05T13:42:11.397729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tiktoken -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38289bb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T13:42:26.570800Z",
     "iopub.status.busy": "2024-09-05T13:42:26.570360Z",
     "iopub.status.idle": "2024-09-05T13:42:29.764089Z",
     "shell.execute_reply": "2024-09-05T13:42:29.762916Z"
    },
    "papermill": {
     "duration": 3.205065,
     "end_time": "2024-09-05T13:42:29.767146",
     "exception": false,
     "start_time": "2024-09-05T13:42:26.562081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea425b8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T13:42:29.781686Z",
     "iopub.status.busy": "2024-09-05T13:42:29.781136Z",
     "iopub.status.idle": "2024-09-05T13:42:29.841805Z",
     "shell.execute_reply": "2024-09-05T13:42:29.840604Z"
    },
    "papermill": {
     "duration": 0.071165,
     "end_time": "2024-09-05T13:42:29.844399",
     "exception": false,
     "start_time": "2024-09-05T13:42:29.773234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import torch.nn as nn\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,emb_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.eps=1e-5\n",
    "        self.scale=nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift=nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self,x):\n",
    "        mean=x.mean(keepdim=True,dim=-1)\n",
    "        var=x.var(keepdim=True,dim=-1,unbiased=False)\n",
    "        norm_value=(x-mean)/torch.sqrt(self.eps+var)  \n",
    "        return self.scale*norm_value+self.shift\n",
    "    \n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return 0.5*x*(1+torch.tanh(torch.sqrt(torch.tensor(2)/torch.tensor(torch.pi))*(x+0.044715*torch.pow(x,3))))\n",
    "    \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.layers=nn.Sequential(\n",
    "            nn.Linear(cfg['emb_dim'],4*cfg['emb_dim']),\n",
    "            GELU(),\n",
    "            nn.Linear(4*cfg['emb_dim'],cfg['emb_dim'])\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_in,d_out,context_length,num_heads,drop=0.5,qkvbias=False) -> None:\n",
    "        super().__init__()\n",
    "        assert (d_out%num_heads==0),\"output dim should be divisible by number of heads\"\n",
    "\n",
    "        self.d_out=d_out\n",
    "\n",
    "        self.w_query=nn.Linear(d_in,d_out,bias=qkvbias)\n",
    "        self.w_key=nn.Linear(d_in,d_out,bias=qkvbias)\n",
    "        self.w_value=nn.Linear(d_in,d_out,bias=qkvbias)\n",
    "\n",
    "        self.register_buffer(\"mask\",torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "        \n",
    "        self.num_heads=num_heads\n",
    "        self.head_dim=d_out//num_heads\n",
    "\n",
    "        self.drop=nn.Dropout(drop)\n",
    "        \n",
    "        #the last layer\n",
    "        self.out_proj=nn.Linear(d_out,d_out,bias=qkvbias)\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        batch,num_tokens,input_dim=x.shape\n",
    "\n",
    "        queries=self.w_query(x)\n",
    "        key=self.w_key(x)\n",
    "        value=self.w_value(x)\n",
    "        \n",
    "        queries=queries.view(batch,num_tokens,self.num_heads,self.head_dim)\n",
    "        key=key.view(batch,num_tokens,self.num_heads,self.head_dim)\n",
    "        value=value.view(batch,num_tokens,self.num_heads,self.head_dim)\n",
    "\n",
    "        #lets transpose \n",
    "        queries=queries.transpose(1,2)\n",
    "        key=key.transpose(1,2)\n",
    "        value=value.transpose(1,2)\n",
    "\n",
    "        attention_score=queries@key.transpose(2,3)\n",
    "\n",
    "        mask_bool=self.mask.bool()[:num_tokens,:num_tokens]\n",
    "        attention_score.masked_fill_(mask_bool,-torch.inf)\n",
    "       \n",
    "        attention_weight=torch.softmax(attention_score/key.shape[-1]**0.5,dim=-1)\n",
    "\n",
    "        attention_weight=self.drop(attention_weight)\n",
    "\n",
    "        context_vector=(attention_weight@value).transpose(1,2)\n",
    "\n",
    "        context_vector=context_vector.contiguous().view(batch,num_tokens,self.d_out)\n",
    "        context_vector=self.out_proj(context_vector)\n",
    "        return context_vector\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.layer_norm1=LayerNorm(cfg['emb_dim'])\n",
    "        self.layer_norm2=LayerNorm(cfg['emb_dim'])\n",
    "\n",
    "        self.attention=MultiHeadAttention(d_in=cfg['emb_dim'],\n",
    "                                      d_out=cfg['emb_dim'],\n",
    "                                      context_length=cfg['context_length'],\n",
    "                                      num_heads=cfg['n_heads'],\n",
    "                                      drop=cfg['drop_rate'],\n",
    "                                      qkvbias=cfg['qkv_bias']) \n",
    "        \n",
    "        self.drop_residual=nn.Dropout(cfg['drop_rate'])\n",
    "        self.feedforward=FeedForward(cfg)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #first block\n",
    "        residual=x  #residual attention\n",
    "        x=self.layer_norm1(x)\n",
    "        x=self.attention(x)\n",
    "        x=self.drop_residual(x)\n",
    "\n",
    "        #lets connect to residual\n",
    "        x=x+residual\n",
    "\n",
    "        #second block\n",
    "        residual=x\n",
    "        x=self.layer_norm2(x)\n",
    "        x=self.feedforward(x)\n",
    "        x=self.drop_residual(x)\n",
    "        x=x+residual\n",
    "\n",
    "        return x\n",
    "    \n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.token_emb=nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])\n",
    "        self.pos_emb=nn.Embedding(cfg['context_length'],cfg['emb_dim'])\n",
    "        self.drop=nn.Dropout(cfg['drop_rate'])\n",
    "        self.transformer_block=nn.Sequential(*[\n",
    "            TransformerBlock(cfg) for _ in range(cfg['n_layers'])\n",
    "        ])\n",
    "        self.last_norm=LayerNorm(cfg['emb_dim'])\n",
    "        self.out_prog=nn.Linear(cfg['emb_dim'],cfg['vocab_size'],bias=False)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #shape\n",
    "        batch,seq_length=x.shape\n",
    "\n",
    "        toke_emb=self.token_emb(x)\n",
    "        pos_emb=self.pos_emb(torch.arange(seq_length,device=x.device))\n",
    "        \n",
    "        x=toke_emb+pos_emb\n",
    "        x=self.drop(x)\n",
    "\n",
    "        x=self.transformer_block(x)\n",
    "        x=self.last_norm(x)\n",
    "\n",
    "        logits=self.out_prog(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def generate_text(model,idx,context_length,new_token):\n",
    "    for _ in range(new_token):\n",
    "        idx=idx[:,-context_length:]\n",
    "        with torch.no_grad():\n",
    "            logits=model(idx)\n",
    "            \n",
    "        logits=logits[:,-1,:] #last token\n",
    "        probs=torch.softmax(logits,dim=-1)\n",
    "        next_word=torch.argmax(probs,dim=-1,keepdim=True)  #token position\n",
    "        idx=torch.cat((idx,next_word),dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02265d33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T13:42:29.859088Z",
     "iopub.status.busy": "2024-09-05T13:42:29.858027Z",
     "iopub.status.idle": "2024-09-05T13:42:29.867430Z",
     "shell.execute_reply": "2024-09-05T13:42:29.866240Z"
    },
    "papermill": {
     "duration": 0.019531,
     "end_time": "2024-09-05T13:42:29.869941",
     "exception": false,
     "start_time": "2024-09-05T13:42:29.850410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GptDataSetv1(Dataset):\n",
    "    def __init__(self,tokenizer,dataset,context_length,stride) -> None:\n",
    "        super().__init__()\n",
    "        self.tokenizer=tokenizer\n",
    "\n",
    "        #lets tokenize the text\n",
    "        self.tokens=self.tokenizer.encode(dataset,allowed_special={\"<|endoftext|>\"})   #array of ids\n",
    "        \n",
    "        self.inputs=[]\n",
    "        self.outputs=[]\n",
    "\n",
    "        for i in range(0,len(self.tokens),stride):\n",
    "            input_chunks=self.tokens[i:i+context_length]\n",
    "            output_chunks=self.tokens[i+1:i+context_length+1]\n",
    "\n",
    "            #lets append\n",
    "            if(len(input_chunks)==context_length and len(output_chunks)==context_length):\n",
    "                self.inputs.append(torch.tensor(input_chunks))\n",
    "                self.outputs.append(torch.tensor(output_chunks))\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index) :\n",
    "        #purpose of this function is to make an input and output matcher\n",
    "        return self.inputs[index].clone().detach(),self.outputs[index].clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63c1c893",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T13:42:29.884170Z",
     "iopub.status.busy": "2024-09-05T13:42:29.883731Z",
     "iopub.status.idle": "2024-09-05T13:42:29.889551Z",
     "shell.execute_reply": "2024-09-05T13:42:29.888532Z"
    },
    "papermill": {
     "duration": 0.016309,
     "end_time": "2024-09-05T13:42:29.892684",
     "exception": false,
     "start_time": "2024-09-05T13:42:29.876375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs,outputs=zip(*batch)\n",
    "    inputs=pad_sequence(inputs,batch_first=True,padding_value=0)\n",
    "    outputs=pad_sequence(outputs,batch_first=True,padding_value=0)\n",
    "    return inputs,outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a4d87bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T13:42:29.908711Z",
     "iopub.status.busy": "2024-09-05T13:42:29.908312Z",
     "iopub.status.idle": "2024-09-05T13:42:29.915147Z",
     "shell.execute_reply": "2024-09-05T13:42:29.913906Z"
    },
    "papermill": {
     "duration": 0.016981,
     "end_time": "2024-09-05T13:42:29.917799",
     "exception": false,
     "start_time": "2024-09-05T13:42:29.900818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt,batch_size=4,context_length=120,stride=128,shuffle=True,drop_last=True):\n",
    "    tokenizer=tiktoken.get_encoding('gpt2')\n",
    "    dataset=GptDataSetv1(tokenizer,txt,context_length,stride)\n",
    "    #prepare the datalaoder\n",
    "    dataloader=DataLoader(dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          collate_fn=collate_fn,\n",
    "                          shuffle=shuffle,\n",
    "                          drop_last=drop_last\n",
    "                         )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59996b1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T13:42:29.931886Z",
     "iopub.status.busy": "2024-09-05T13:42:29.931466Z",
     "iopub.status.idle": "2024-09-05T13:42:29.937068Z",
     "shell.execute_reply": "2024-09-05T13:42:29.935755Z"
    },
    "papermill": {
     "duration": 0.015647,
     "end_time": "2024-09-05T13:42:29.939565",
     "exception": false,
     "start_time": "2024-09-05T13:42:29.923918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256, \n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67ca56c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T13:42:29.954875Z",
     "iopub.status.busy": "2024-09-05T13:42:29.954417Z",
     "iopub.status.idle": "2024-09-05T13:42:33.682718Z",
     "shell.execute_reply": "2024-09-05T13:42:33.681573Z"
    },
    "papermill": {
     "duration": 3.739664,
     "end_time": "2024-09-05T13:42:33.685298",
     "exception": false,
     "start_time": "2024-09-05T13:42:29.945634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer=tiktoken.get_encoding('gpt2')\n",
    "\n",
    "def text_to_ids(text,tokenizer):\n",
    "    #this convert text into token ids\n",
    "    \n",
    "    encoded=tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor=torch.tensor(encoded)\n",
    "    encoded_tensor=encoded_tensor.unsqueeze(dim=0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def ids_to_text(ids,tokenizer):\n",
    "    #this converts the tokens ids into text\n",
    "    return tokenizer.decode(ids.squeeze(dim=0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87abe037",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T13:42:33.699044Z",
     "iopub.status.busy": "2024-09-05T13:42:33.698578Z",
     "iopub.status.idle": "2024-09-05T13:42:33.704646Z",
     "shell.execute_reply": "2024-09-05T13:42:33.703422Z"
    },
    "papermill": {
     "duration": 0.015804,
     "end_time": "2024-09-05T13:42:33.707058",
     "exception": false,
     "start_time": "2024-09-05T13:42:33.691254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for calculating the loss of a single batch\n",
    "def loss_batch(inputs,target,model,device):\n",
    "    #lets move all varaible into the same device\n",
    "    inputs,target=inputs.to(device),target.to(device)\n",
    "    \n",
    "    logits=model(inputs)    \n",
    "   \n",
    "    loss=torch.nn.functional.cross_entropy(logits.flatten(0,1),target.flatten())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4336d16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T13:42:33.720647Z",
     "iopub.status.busy": "2024-09-05T13:42:33.720173Z",
     "iopub.status.idle": "2024-09-05T13:42:33.727927Z",
     "shell.execute_reply": "2024-09-05T13:42:33.726572Z"
    },
    "papermill": {
     "duration": 0.017598,
     "end_time": "2024-09-05T13:42:33.730575",
     "exception": false,
     "start_time": "2024-09-05T13:42:33.712977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#lets calculate the loss for the whole batch\n",
    "def total_loss_batches(dataloader,model,device,num_batches=None):\n",
    "    if(num_batches==None):\n",
    "        num_batches=len(dataloader)\n",
    "    else:\n",
    "        num_batches=min(num_batches,len(dataloader))\n",
    "    \n",
    "    #lets calculate the loss over batches\n",
    "    total_loss=0.\n",
    "    for i,(inputs,target) in enumerate(dataloader):\n",
    "        if(i<num_batches):\n",
    "            loss=loss_batch(inputs,target,model,device)\n",
    "            total_loss+=loss.item()\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    total_loss=total_loss/num_batches\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "784663d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T13:42:33.744822Z",
     "iopub.status.busy": "2024-09-05T13:42:33.743844Z",
     "iopub.status.idle": "2024-09-05T13:42:33.750933Z",
     "shell.execute_reply": "2024-09-05T13:42:33.749672Z"
    },
    "papermill": {
     "duration": 0.017122,
     "end_time": "2024-09-05T13:42:33.753708",
     "exception": false,
     "start_time": "2024-09-05T13:42:33.736586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#generate new tokens\n",
    "def generate_new_tokens(model,device,start_context,tokenizer,max_tokens=50):\n",
    "    model.eval()\n",
    "    ids=text_to_ids(start_context,tokenizer).to(device)\n",
    "    context_length=model.pos_emb.weight.shape[0]\n",
    "    new_ids=generate_text(model,ids,context_length,max_tokens)\n",
    "    \n",
    "    #convert idx into text\n",
    "    with torch.no_grad():\n",
    "        new_text=ids_to_text(new_ids,tokenizer)\n",
    "        \n",
    "    model.train()\n",
    "    print(f\"\\n {new_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5853847f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T13:42:33.768295Z",
     "iopub.status.busy": "2024-09-05T13:42:33.767853Z",
     "iopub.status.idle": "2024-09-05T13:42:33.778011Z",
     "shell.execute_reply": "2024-09-05T13:42:33.776630Z"
    },
    "papermill": {
     "duration": 0.020596,
     "end_time": "2024-09-05T13:42:33.780456",
     "exception": false,
     "start_time": "2024-09-05T13:42:33.759860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(model,idx,context_length,new_token_length,device,temprature,topk):\n",
    "    idx_cont=idx[:,-context_length:] #2d inputs num of tokens by embeding dim\n",
    "    for _ in range(new_token_length):\n",
    "        with torch.no_grad():\n",
    "            logits=model(idx_cont)\n",
    "\n",
    "        #lets apply topk\n",
    "        logits=logits[:, -1, :]  #take only last tokens prediction\n",
    "        if(topk is not None):\n",
    "            top_logits,_=torch.topk(logits,k=topk)\n",
    "            min_value=top_logits[:,-1]\n",
    "            \n",
    "            logits=torch.where(\n",
    "                logits<min_value.unsqueeze(dim=-1),\n",
    "                torch.tensor(float('-inf')).to(device),\n",
    "                logits\n",
    "            )\n",
    "        \n",
    "        #lets apply multinomial\n",
    "        if(temprature>0.0):\n",
    "            logits=logits/temprature\n",
    "            probs=torch.softmax(logits,dim=-1)\n",
    "            next_token=torch.multinomial(probs,num_samples=1)\n",
    "        else:\n",
    "            probs=torch.softmax(logits,dim=-1)\n",
    "            next_token=torch.argamax(probs,dim=-1,keepdim=True)\n",
    "            \n",
    "        idx=torch.cat((idx,next_token),dim=1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdb4fbe",
   "metadata": {
    "papermill": {
     "duration": 0.006171,
     "end_time": "2024-09-05T13:42:33.792647",
     "exception": false,
     "start_time": "2024-09-05T13:42:33.786476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7126b46",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-09-05T13:42:33.807204Z",
     "iopub.status.busy": "2024-09-05T13:42:33.805894Z",
     "iopub.status.idle": "2024-09-05T13:42:34.017899Z",
     "shell.execute_reply": "2024-09-05T13:42:34.016646Z"
    },
    "papermill": {
     "duration": 0.221961,
     "end_time": "2024-09-05T13:42:34.020694",
     "exception": false,
     "start_time": "2024-09-05T13:42:33.798733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_download.py', <http.client.HTTPMessage at 0x7c01498bd690>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = (\n",
    "\"https://raw.githubusercontent.com/rasbt/\"\n",
    "\"LLMs-from-scratch/main/ch05/\"\n",
    "\"01_main-chapter-code/gpt_download.py\"\n",
    ")\n",
    "filename = url.split('/')[-1]\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d36d314",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T13:42:34.035485Z",
     "iopub.status.busy": "2024-09-05T13:42:34.035025Z",
     "iopub.status.idle": "2024-09-05T13:42:49.369940Z",
     "shell.execute_reply": "2024-09-05T13:42:49.368616Z"
    },
    "papermill": {
     "duration": 15.345457,
     "end_time": "2024-09-05T13:42:49.372750",
     "exception": false,
     "start_time": "2024-09-05T13:42:34.027293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gpt_download import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b195235",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T13:42:49.387160Z",
     "iopub.status.busy": "2024-09-05T13:42:49.386445Z",
     "iopub.status.idle": "2024-09-05T13:43:47.472060Z",
     "shell.execute_reply": "2024-09-05T13:43:47.470957Z"
    },
    "papermill": {
     "duration": 58.095782,
     "end_time": "2024-09-05T13:43:47.474741",
     "exception": false,
     "start_time": "2024-09-05T13:42:49.378959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 32.7kiB/s]\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [00:01<00:00, 873kiB/s]\n",
      "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 48.2kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [00:51<00:00, 9.73MiB/s]\n",
      "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 2.04MiB/s]\n",
      "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 603kiB/s]\n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 512kiB/s]\n"
     ]
    }
   ],
   "source": [
    "settings,params=download_and_load_gpt2(model_size=\"124M\",models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e94a9f39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T13:43:47.554170Z",
     "iopub.status.busy": "2024-09-05T13:43:47.553695Z",
     "iopub.status.idle": "2024-09-05T13:43:47.559990Z",
     "shell.execute_reply": "2024-09-05T13:43:47.558807Z"
    },
    "papermill": {
     "duration": 0.048218,
     "end_time": "2024-09-05T13:43:47.562516",
     "exception": false,
     "start_time": "2024-09-05T13:43:47.514298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "        \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\":\n",
    "        12},\n",
    "        \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\":\n",
    "        16},\n",
    "        \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\":\n",
    "        20},\n",
    "        \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb780221",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T13:43:47.640096Z",
     "iopub.status.busy": "2024-09-05T13:43:47.639646Z",
     "iopub.status.idle": "2024-09-05T13:43:47.645734Z",
     "shell.execute_reply": "2024-09-05T13:43:47.644444Z"
    },
    "papermill": {
     "duration": 0.047673,
     "end_time": "2024-09-05T13:43:47.648064",
     "exception": false,
     "start_time": "2024-09-05T13:43:47.600391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = config.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024})\n",
    "NEW_CONFIG.update({\"qkv_bias\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7adaa0c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T13:43:47.725560Z",
     "iopub.status.busy": "2024-09-05T13:43:47.725118Z",
     "iopub.status.idle": "2024-09-05T13:43:49.761155Z",
     "shell.execute_reply": "2024-09-05T13:43:49.759880Z"
    },
    "papermill": {
     "duration": 2.078049,
     "end_time": "2024-09-05T13:43:49.763906",
     "exception": false,
     "start_time": "2024-09-05T13:43:47.685857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt=GPTModel(NEW_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1438a9c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T13:43:49.844610Z",
     "iopub.status.busy": "2024-09-05T13:43:49.844177Z",
     "iopub.status.idle": "2024-09-05T13:43:49.851039Z",
     "shell.execute_reply": "2024-09-05T13:43:49.849638Z"
    },
    "papermill": {
     "duration": 0.050833,
     "end_time": "2024-09-05T13:43:49.853532",
     "exception": false,
     "start_time": "2024-09-05T13:43:49.802699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8643e781",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T13:43:49.932577Z",
     "iopub.status.busy": "2024-09-05T13:43:49.932134Z",
     "iopub.status.idle": "2024-09-05T13:43:50.263085Z",
     "shell.execute_reply": "2024-09-05T13:43:50.261931Z"
    },
    "papermill": {
     "duration": 0.373658,
     "end_time": "2024-09-05T13:43:50.265727",
     "exception": false,
     "start_time": "2024-09-05T13:43:49.892069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.token_emb.weight = assign(gpt.token_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.transformer_block[b].attention.w_query.weight = assign(\n",
    "            gpt.transformer_block[b].attention.w_query.weight, q_w.T)\n",
    "        gpt.transformer_block[b].attention.w_query.weight = assign(\n",
    "            gpt.transformer_block[b].attention.w_key.weight, k_w.T)\n",
    "        gpt.transformer_block[b].attention.w_value.weight = assign(\n",
    "            gpt.transformer_block[b].attention.w_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.transformer_block[b].attention.w_query.bias = assign(\n",
    "            gpt.transformer_block[b].attention.w_query.bias, q_b)\n",
    "        gpt.transformer_block[b].attention.w_key.bias = assign(\n",
    "            gpt.transformer_block[b].attention.w_key.bias, k_b)\n",
    "        gpt.transformer_block[b].attention.w_value.bias = assign(\n",
    "            gpt.transformer_block[b].attention.w_value.bias, v_b)\n",
    "\n",
    "        gpt.transformer_block[b].attention.out_proj.weight = assign(\n",
    "            gpt.transformer_block[b].attention.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.transformer_block[b].attention.out_proj.bias = assign(\n",
    "            gpt.transformer_block[b].attention.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.transformer_block[b].feedforward.layers[0].weight = assign(\n",
    "            gpt.transformer_block[b].feedforward.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.transformer_block[b].feedforward.layers[0].bias = assign(\n",
    "            gpt.transformer_block[b].feedforward.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.transformer_block[b].feedforward.layers[2].weight = assign(\n",
    "            gpt.transformer_block[b].feedforward.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.transformer_block[b].feedforward.layers[2].bias = assign(\n",
    "            gpt.transformer_block[b].feedforward.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.transformer_block[b].layer_norm1.scale = assign(\n",
    "            gpt.transformer_block[b].layer_norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.transformer_block[b].layer_norm1.shift = assign(\n",
    "            gpt.transformer_block[b].layer_norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.transformer_block[b].layer_norm2.scale = assign(\n",
    "            gpt.transformer_block[b].layer_norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.transformer_block[b].layer_norm2.shift = assign(\n",
    "            gpt.transformer_block[b].layer_norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.last_norm.scale = assign(gpt.last_norm.scale, params[\"g\"])\n",
    "    gpt.last_norm.shift = assign(gpt.last_norm.shift, params[\"b\"])\n",
    "    gpt.out_prog.weight = assign(gpt.out_prog.weight, params[\"wte\"])\n",
    "    \n",
    "    \n",
    "load_weights_into_gpt(gpt, params)\n",
    "#gpt=gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c36b609",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T13:43:50.343081Z",
     "iopub.status.busy": "2024-09-05T13:43:50.342656Z",
     "iopub.status.idle": "2024-09-05T13:43:54.180410Z",
     "shell.execute_reply": "2024-09-05T13:43:54.179082Z"
    },
    "papermill": {
     "duration": 3.87964,
     "end_time": "2024-09-05T13:43:54.182906",
     "exception": false,
     "start_time": "2024-09-05T13:43:50.303266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the new generated text is:humanity is a the the, the a the that the the a the the a the that a the, the the the am the is the a the the the, the not, the the the the, a the and the,,, the is the also\n"
     ]
    }
   ],
   "source": [
    "new_idx=generate(\n",
    "    gpt,\n",
    "    idx=text_to_ids(\"humanity is\",tokenizer),\n",
    "    context_length=config['context_length'],\n",
    "    new_token_length=50,\n",
    "    device=torch.device('cpu'),\n",
    "    temprature=0.1,\n",
    "    topk=25\n",
    ")\n",
    "\n",
    "print(f\"the new generated text is:{ids_to_text(new_idx,tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f7b93b",
   "metadata": {
    "papermill": {
     "duration": 0.039161,
     "end_time": "2024-09-05T13:43:54.261385",
     "exception": false,
     "start_time": "2024-09-05T13:43:54.222224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 108.554336,
   "end_time": "2024-09-05T13:43:56.942861",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-05T13:42:08.388525",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
